#!/usr/bin/env python3
"""
InvestiGator - LLM Facade Pattern Implementation
Copyright (c) 2025 Vijaykumar Singh
Licensed under the Apache License 2.0

LLM Facade - Simplified interface for pattern-based LLM operations
Provides a clean API for all LLM processing needs
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

from .llm_interfaces import (
    LLMRequest, LLMResponse, LLMTaskType, LLMPriority,
    ILLMStrategy, ILLMProcessor, ILLMObserver, ILLMAnalysisTemplate
)
from .llm_strategies import (
    ComprehensiveLLMStrategy, QuickLLMStrategy,
    StandardLLMCacheStrategy, AggressiveLLMCacheStrategy
)
from .llm_processors import QueuedLLMProcessor, StandardLLMAnalysisTemplate

logger = logging.getLogger(__name__)

class LLMAnalysisObserver(ILLMObserver):
    """Default observer for LLM processing events"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def on_request_queued(self, request: LLMRequest) -> None:
        """Called when request is added to queue"""
        self.logger.debug(f"LLM request queued: {request.request_id} for {request.metadata.get('symbol', 'UNKNOWN') if request.metadata else 'UNKNOWN'}")
    
    def on_processing_started(self, request: LLMRequest) -> None:
        """Called when processing starts"""
        symbol = request.metadata.get('symbol', 'UNKNOWN') if request.metadata else 'UNKNOWN'
        task_type = request.metadata.get('task_type', 'unknown') if request.metadata else 'unknown'
        self.logger.info(f"Starting {task_type} analysis for {symbol}")
    
    def on_processing_completed(self, request: LLMRequest, response: LLMResponse) -> None:
        """Called when processing completes"""
        symbol = request.metadata.get('symbol', 'UNKNOWN') if request.metadata else 'UNKNOWN'
        task_type = request.metadata.get('task_type', 'unknown') if request.metadata else 'unknown'
        self.logger.info(f"Completed {task_type} analysis for {symbol} in {response.processing_time_ms}ms")
    
    def on_processing_error(self, request: LLMRequest, error: Exception) -> None:
        """Called when processing fails"""
        symbol = request.metadata.get('symbol', 'UNKNOWN') if request.metadata else 'UNKNOWN'
        task_type = request.metadata.get('task_type', 'unknown') if request.metadata else 'unknown'
        self.logger.error(f"Failed {task_type} analysis for {symbol}: {error}")

class LLMFacade:
    """
    Facade for simplified LLM operations using design patterns
    Replaces the functionality of ollama_interface.py
    """
    
    def __init__(self, config, cache_manager=None, strategy_type: str = "comprehensive"):
        """
        Initialize LLM facade with configuration
        
        Args:
            config: Configuration object
            cache_manager: Optional cache manager for response caching
            strategy_type: Type of LLM strategy ("comprehensive" or "quick")
        """
        self.config = config
        self.cache_manager = cache_manager
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Create strategy and cache strategy
        self.strategy = self._create_strategy(strategy_type)
        self.cache_strategy = StandardLLMCacheStrategy(config) if cache_manager else None
        
        # Create processor with observer
        self.processor = QueuedLLMProcessor(
            config=config,
            num_threads=1,  # Sequential processing to prevent GPU/RAM exhaustion
            cache_manager=cache_manager,
            cache_strategy=self.cache_strategy
        )
        
        # Add observer for logging
        self.observer = LLMAnalysisObserver()
        self.processor.attach(self.observer)
        
        # Create analysis template
        self.analysis_template = StandardLLMAnalysisTemplate(self.processor, self.strategy)
        
        self.logger.info(f"LLM Facade initialized with {strategy_type} strategy")
    
    def _create_strategy(self, strategy_type: str) -> ILLMStrategy:
        """Create appropriate strategy based on type"""
        if strategy_type == "comprehensive":
            return ComprehensiveLLMStrategy(self.config)
        elif strategy_type == "quick":
            return QuickLLMStrategy(self.config)
        else:
            self.logger.warning(f"Unknown strategy type '{strategy_type}', using comprehensive")
            return ComprehensiveLLMStrategy(self.config)
    
    # ============================================================================
    # High-Level Analysis Methods (Template Method Pattern)
    # ============================================================================
    
    def analyze_fundamental(self, symbol: str, quarterly_data: List[Dict], filing_data: Dict = None) -> Dict[str, Any]:
        """
        Perform fundamental analysis using template method pattern
        
        Args:
            symbol: Stock symbol
            quarterly_data: List of quarterly financial data
            filing_data: Optional filing information
            
        Returns:
            Structured analysis result
        """
        try:
            data = {
                'symbol': symbol,
                'quarterly_data': quarterly_data,
                'filing_data': filing_data or {}
            }
            
            return self.analysis_template.analyze(symbol, data, LLMTaskType.FUNDAMENTAL_ANALYSIS)
            
        except Exception as e:
            self.logger.error(f"Error in fundamental analysis: {e}")
            return {
                'error': str(e),
                'symbol': symbol,
                'task_type': 'fundamental_analysis',
                'timestamp': datetime.utcnow().isoformat()
            }
    
    def analyze_technical(self, symbol: str, price_data: Dict, indicators: Dict = None) -> Dict[str, Any]:
        """
        Perform technical analysis using template method pattern
        
        Args:
            symbol: Stock symbol
            price_data: Price and volume data
            indicators: Technical indicators
            
        Returns:
            Structured analysis result
        """
        try:
            data = {
                'symbol': symbol,
                'price_data': price_data,
                'indicators': indicators or {}
            }
            
            return self.analysis_template.analyze(symbol, data, LLMTaskType.TECHNICAL_ANALYSIS)
            
        except Exception as e:
            self.logger.error(f"Error in technical analysis: {e}")
            return {
                'error': str(e),
                'symbol': symbol,
                'task_type': 'technical_analysis',
                'timestamp': datetime.utcnow().isoformat()
            }
    
    def synthesize_analysis(self, symbol: str, fundamental_result: Dict, technical_result: Dict) -> Dict[str, Any]:
        """
        Synthesize fundamental and technical analysis
        
        Args:
            symbol: Stock symbol
            fundamental_result: Fundamental analysis result
            technical_result: Technical analysis result
            
        Returns:
            Synthesized analysis result
        """
        try:
            data = {
                'symbol': symbol,
                'fundamental_analysis': fundamental_result,
                'technical_analysis': technical_result
            }
            
            return self.analysis_template.analyze(symbol, data, LLMTaskType.SYNTHESIS)
            
        except Exception as e:
            self.logger.error(f"Error in synthesis: {e}")
            return {
                'error': str(e),
                'symbol': symbol,
                'task_type': 'synthesis',
                'timestamp': datetime.utcnow().isoformat()
            }
    
    def analyze_quarterly_summary(self, symbol: str, quarter_data: Dict) -> Dict[str, Any]:
        """
        Create quarterly performance summary
        
        Args:
            symbol: Stock symbol
            quarter_data: Single quarter financial data
            
        Returns:
            Quarterly summary result
        """
        try:
            data = {
                'symbol': symbol,
                'quarter_data': quarter_data
            }
            
            return self.analysis_template.analyze(symbol, data, LLMTaskType.QUARTERLY_SUMMARY)
            
        except Exception as e:
            self.logger.error(f"Error in quarterly summary: {e}")
            return {
                'error': str(e),
                'symbol': symbol,
                'task_type': 'quarterly_summary',
                'timestamp': datetime.utcnow().isoformat()
            }
    
    def assess_risks(self, symbol: str, all_data: Dict) -> Dict[str, Any]:
        """
        Perform comprehensive risk assessment
        
        Args:
            symbol: Stock symbol
            all_data: Combined analysis data
            
        Returns:
            Risk assessment result
        """
        try:
            data = dict(all_data)
            data['symbol'] = symbol
            
            return self.analysis_template.analyze(symbol, data, LLMTaskType.RISK_ASSESSMENT)
            
        except Exception as e:
            self.logger.error(f"Error in risk assessment: {e}")
            return {
                'error': str(e),
                'symbol': symbol,
                'task_type': 'risk_assessment',
                'timestamp': datetime.utcnow().isoformat()
            }
    
    # ============================================================================
    # Direct LLM Methods (Strategy Pattern)
    # ============================================================================
    
    def generate_response(self, task_type: LLMTaskType, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate LLM response using strategy pattern
        
        Args:
            task_type: Type of analysis task
            data: Input data for analysis
            
        Returns:
            Processed LLM response
        """
        try:
            # Prepare request using strategy
            request = self.strategy.prepare_request(task_type, data)
            
            # Process request through processor
            response = self.processor.process_request(request)
            
            # Process response using strategy
            return self.strategy.process_response(response, task_type)
            
        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            return {
                'error': str(e),
                'task_type': task_type.value if isinstance(task_type, LLMTaskType) else str(task_type),
                'timestamp': datetime.utcnow().isoformat()
            }
    
    def generate_batch_responses(self, requests: List[Dict]) -> List[Dict[str, Any]]:
        """
        Generate multiple LLM responses in batch
        
        Args:
            requests: List of request dictionaries with 'task_type' and 'data' keys
            
        Returns:
            List of processed responses
        """
        try:
            llm_requests = []
            
            for req in requests:
                task_type = LLMTaskType(req['task_type'])
                data = req['data']
                llm_request = self.strategy.prepare_request(task_type, data)
                llm_requests.append((llm_request, task_type))
            
            # Process batch
            llm_responses = self.processor.process_batch([req for req, _ in llm_requests])
            
            # Process responses
            results = []
            for i, response in enumerate(llm_responses):
                _, task_type = llm_requests[i]
                result = self.strategy.process_response(response, task_type)
                results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error in batch processing: {e}")
            return [{'error': str(e), 'timestamp': datetime.utcnow().isoformat()}] * len(requests)
    
    # ============================================================================
    # Legacy Compatibility Methods
    # ============================================================================
    
    def query_ollama(self, model: str, prompt: str, system_prompt: str = None, **kwargs) -> Dict[str, Any]:
        """
        Legacy compatibility method for direct Ollama queries
        Maintains backward compatibility with existing code
        
        Args:
            model: Model name
            prompt: User prompt
            system_prompt: Optional system prompt
            **kwargs: Additional parameters
            
        Returns:
            Response dictionary
        """
        try:
            # Create direct LLM request
            request = LLMRequest(
                model=model,
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=kwargs.get('temperature', 0.3),
                top_p=kwargs.get('top_p', 0.9),
                num_ctx=kwargs.get('num_ctx'),
                num_predict=kwargs.get('num_predict'),
                timeout=kwargs.get('timeout'),
                metadata=kwargs.get('metadata', {}),
                priority=kwargs.get('priority', LLMPriority.NORMAL.value),
                request_id=kwargs.get('request_id')
            )
            
            # Process request
            response = self.processor.process_request(request)
            
            # Return in legacy format
            return {
                'response': response.content,
                'model': response.model,
                'processing_time_ms': response.processing_time_ms,
                'tokens_used': response.tokens_used,
                'error': response.error,
                'metadata': response.metadata
            }
            
        except Exception as e:
            self.logger.error(f"Error in legacy query: {e}")
            return {
                'response': '',
                'model': model,
                'processing_time_ms': 0,
                'error': str(e)
            }
    
    def generate(self, model: str, prompt: str, system_prompt: str = None, **kwargs) -> str:
        """
        Legacy compatibility method that returns just the response content
        Maintains backward compatibility with existing code that expects a string response
        
        Args:
            model: Model name
            prompt: User prompt
            system_prompt: Optional system prompt
            **kwargs: Additional parameters
            
        Returns:
            Response content as string
        """
        try:
            result = self.query_ollama(model, prompt, system_prompt, **kwargs)
            return result.get('response', '')
            
        except Exception as e:
            self.logger.error(f"Error in legacy query: {e}")
            return ''
    
    # ============================================================================
    # Utility Methods
    # ============================================================================
    
    def get_queue_status(self) -> Dict[str, Any]:
        """Get current processing queue status"""
        return {
            'queue_size': self.processor.get_queue_size(),
            'strategy': self.strategy.get_strategy_name(),
            'cache_enabled': self.cache_manager is not None,
            'timestamp': datetime.utcnow().isoformat()
        }
    
    def shutdown(self):
        """Shutdown the LLM facade and cleanup resources"""
        try:
            self.processor.detach(self.observer)
            self.processor.stop()
            self.logger.info("LLM Facade shutdown completed")
        except Exception as e:
            self.logger.error(f"Error during shutdown: {e}")
    
    def __enter__(self):
        """Context manager entry"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.shutdown()

# ============================================================================
# Factory Functions for Easy Usage
# ============================================================================

def create_llm_facade(config, cache_manager=None, strategy_type: str = "comprehensive") -> LLMFacade:
    """
    Factory function to create LLM facade
    
    Args:
        config: Configuration object
        cache_manager: Optional cache manager
        strategy_type: Strategy type ("comprehensive" or "quick")
        
    Returns:
        Configured LLM facade instance
    """
    return LLMFacade(config, cache_manager, strategy_type)

def create_quick_llm_facade(config, cache_manager=None) -> LLMFacade:
    """
    Factory function for quick analysis LLM facade
    
    Args:
        config: Configuration object
        cache_manager: Optional cache manager
        
    Returns:
        Quick analysis LLM facade
    """
    return LLMFacade(config, cache_manager, "quick")

def create_comprehensive_llm_facade(config, cache_manager=None) -> LLMFacade:
    """
    Factory function for comprehensive analysis LLM facade
    
    Args:
        config: Configuration object
        cache_manager: Optional cache manager
        
    Returns:
        Comprehensive analysis LLM facade
    """
    return LLMFacade(config, cache_manager, "comprehensive")