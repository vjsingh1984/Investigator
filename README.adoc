// InvestiGator - AI Investment Research Assistant
// Copyright (c) 2025 Vijaykumar Singh
// Licensed under the Apache License, Version 2.0
// See LICENSE file for details

= 🐊 InvestiGator - AI Investment Research Assistant
Vijaykumar Singh <https://github.com/vjsingh1984>
v2.1, 2025-01-31
:doctype: book
:toc: left
:toclevels: 3
:sectanchors:
:sectlinks:
:sectnums:
:source-highlighter: highlight.js
:icons: font
:imagesdir: images
:reproducible:

[.lead]
*Your AI-powered investment research companion that runs entirely on your MacBook*

image:https://img.shields.io/badge/License-Apache_2.0-blue.svg[License: Apache 2.0,link=https://opensource.org/licenses/Apache-2.0]
image:https://img.shields.io/badge/python-3.9+-blue.svg[Python 3.9+,link=https://www.python.org/downloads/]
image:https://img.shields.io/badge/macOS-12.0+-000000.svg?logo=apple[macOS,link=https://www.apple.com/macos/]
image:https://img.shields.io/badge/Apple%20Silicon-M1%2FM2%2FM3-ff6b35.svg[Apple Silicon,link=https://www.apple.com/mac/]
image:https://img.shields.io/badge/SEC%20EDGAR-Free%20API-007ACC.svg[SEC EDGAR,link=https://www.sec.gov/edgar/sec-api-documentation]

*InvestiGator* is a sophisticated AI investment research system that combines SEC filing analysis with technical analysis to generate professional investment recommendations. Built for privacy-conscious investors who want institutional-grade analysis without cloud dependencies.

IMPORTANT: *No API Key Required!* InvestiGator uses free SEC EDGAR APIs and Yahoo Finance for all financial data.

== 📋 Table of Contents

* <<introduction,Introduction>>
* <<features,Features>>
* <<architecture,System Architecture>>
* <<data-flow,Data Flow & Processing>>
* <<cache-management,Cache Management System>>
* <<cache-structure,Cache & Observability>>
* <<prerequisites,Prerequisites>>
* <<quick-start,Quick Start>>
* <<detailed-setup,Detailed Setup>>
* <<usage,Usage Guide>>
* <<configuration,Configuration>>
* <<testing,Testing>>
* <<troubleshooting,Troubleshooting>>
* <<security,Security & Privacy>>
* <<contributing,Contributing>>
* <<license,License>>

[[introduction]]
== 🌟 Introduction

InvestiGator is a comprehensive investment research platform that runs entirely on your local machine. It combines:

* *SEC EDGAR Integration*: Direct access to company filings using free APIs
* *Technical Analysis*: Real-time market data analysis with advanced indicators
* *AI-Powered Insights*: Three specialized LLMs for different analysis aspects
* *Privacy-First Design*: All processing happens locally on your Mac
* *Professional Reports*: Institutional-quality PDF reports with actionable insights

[[features]]
== ✨ Key Features

=== 🔍 Comprehensive Analysis
* *SEC Filing Analysis*: AI-powered fundamental analysis of 10-K/10-Q filings using pattern-based architecture
* *XBRL Data Processing*: Automated extraction of financial metrics via SEC Frame API
* *Technical Analysis*: 30+ indicators including moving averages, RSI, MACD, Bollinger Bands, Fibonacci levels
* *Investment Synthesis*: Weighted recommendations (60% fundamental, 40% technical) with 0-10 scoring
* *Professional Reports*: PDF reports with charts, executive summaries, and actionable insights

=== 🛡️ Privacy-First Design
* *100% Local Processing*: All AI models run on your MacBook using Ollama
* *No Cloud Dependencies*: Your investment data never leaves your device
* *No API Keys Required*: Uses free SEC EDGAR APIs and Yahoo Finance
* *Secure Storage*: PostgreSQL with connection pooling and encrypted credentials
* *Private Communications*: Optional secure email delivery via SMTP/TLS

=== 🤖 Advanced AI Integration
* *Three-Stage LLM Pipeline*: SEC fundamental → Technical analysis → Investment synthesis
* *Configurable Models*: Support for Llama3.1, Mixtral, Phi4, Qwen2.5, and custom models
* *Single-Threaded Processing*: Prevents GPU/RAM exhaustion with sequential execution
* *Complete LLM Observability*: All prompts and responses cached with processing metrics
* *Large Context Support*: Up to 32K context window for comprehensive analysis
* *Structured JSON Outputs*: Consistent, parseable AI responses

=== 💾 Intelligent Cache Management
* *Multi-Level Caching*: Disk and database storage with configurable backends
* *Uniform Compression*: gzip (level 9) for disk, JSONB compression for PostgreSQL
* *Flexible Configuration*: Enable/disable specific storage types and cache categories
* *Cache Cleanup & Inspection*: Comprehensive utilities for cache management
* *Force Refresh Capability*: Global or symbol-specific cache invalidation
* *Performance Optimization*: Intelligent cache prioritization and hit-rate tracking

=== ⚡ Performance & Automation
* *Multi-Backend Caching*: Memory (priority 30) → Disk (priority 10) → Database (priority 5)
* *Smart Cache Management*: Automatic promotion of frequently accessed data to faster storage
* *Efficient Data Storage*: 70-80% compression ratios with gzip level 9
* *Weekly Reports*: Automated portfolio analysis with batch processing
* *Configurable Processing*: Single-threaded LLM execution to prevent resource exhaustion
* *Cache Performance*: 10-50ms disk access, 50-200ms database access
* *Extensible Architecture*: Pattern-based design with facades, strategies, and observers

[[architecture]]
== 🏗️ System Architecture

[mermaid]
....
graph TB
    subgraph "Entry Point"
        SHELL["🐢 investigator.sh<br/>Bash Orchestrator"]
        CONFIG["⚙️ config.json<br/>Central Configuration"]
    end
    
    subgraph "Core Modules"
        FUND["📊 sec_fundamental.py<br/>Pattern-based Facade"]
        TECH["📈 yahoo_technical.py<br/>Technical Analysis"]
        SYNTH["🔗 synthesizer.py<br/>Report Generation"]
    end
    
    subgraph "Pattern Architecture"
        SEC_FACADE["🏛️ SEC Facade<br/>patterns.sec.sec_facade"]
        LLM_FACADE["🤖 LLM Facade<br/>patterns.llm.llm_facade"]
        STRATEGIES["📐 Strategies<br/>Analysis Patterns"]
    end
    
    subgraph "Data Sources"
        SEC_API["🏛️ SEC EDGAR<br/>Free APIs"]
        YAHOO_API["📈 Yahoo Finance<br/>yfinance library"]
        TICKER_MAP["🏷️ Ticker Mapper<br/>SEC ticker.txt"]
    end
    
    subgraph "Cache System"
        CACHE_MGR["🎯 CacheManager<br/>utils.cache.cache_manager"]
        DISK["💾 Disk Cache<br/>JSON + gzip(9)"]
        DB_CACHE["🗄️ DB Cache<br/>PostgreSQL JSONB"]
        MEM["🧠 Memory Cache<br/>In-Process"]
    end
    
    subgraph "AI Processing"
        OLLAMA["🦙 Ollama<br/>Local LLM Server"]
        LLAMA31["📝 llama3.1:8b<br/>All Analysis Types"]
    end
    
    subgraph "Data Layer"
        DB[(🗄️ PostgreSQL<br/>investment_ai DB)]
        DAOS["📦 DAOs<br/>Data Access Objects"]
    end
    
    subgraph "Outputs"
        PDF["📄 PDF Reports<br/>ReportLab + Charts"]
        EMAIL["📧 Email<br/>SMTP/TLS Optional"]
        LOGS["📋 Logs<br/>Symbol-specific"]
    end
    
    %% Main Flow
    SHELL --> CONFIG
    SHELL --> FUND
    SHELL --> TECH
    SHELL --> SYNTH
    
    %% Pattern connections
    FUND --> SEC_FACADE
    SEC_FACADE --> LLM_FACADE
    SEC_FACADE --> STRATEGIES
    
    %% Data source connections
    SEC_FACADE --> TICKER_MAP
    TICKER_MAP --> SEC_API
    TECH --> YAHOO_API
    
    %% Cache flow
    SEC_API --> CACHE_MGR
    YAHOO_API --> CACHE_MGR
    CACHE_MGR --> MEM
    CACHE_MGR --> DISK
    CACHE_MGR --> DB_CACHE
    
    %% AI connections
    LLM_FACADE --> OLLAMA
    OLLAMA --> LLAMA31
    TECH --> OLLAMA
    SYNTH --> OLLAMA
    
    %% Database connections
    DB_CACHE --> DB
    DAOS --> DB
    FUND --> DAOS
    TECH --> DAOS
    SYNTH --> DAOS
    
    %% Output connections
    SYNTH --> PDF
    SYNTH --> EMAIL
    SHELL --> LOGS
    
    %% Styling
    classDef entry fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
    classDef core fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    classDef pattern fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
    classDef external fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    classDef cache fill:#fce4ec,stroke:#e91e63,stroke-width:2px
    classDef ai fill:#e0f2f1,stroke:#009688,stroke-width:2px
    classDef storage fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    classDef output fill:#fffde7,stroke:#ffeb3b,stroke-width:2px
    
    class SHELL,CONFIG entry
    class FUND,TECH,SYNTH core
    class SEC_FACADE,LLM_FACADE,STRATEGIES pattern
    class SEC_API,YAHOO_API,TICKER_MAP external
    class CACHE_MGR,DISK,DB_CACHE,MEM cache
    class OLLAMA,LLAMA31 ai
    class DB,DAOS storage
    class PDF,EMAIL,LOGS output
....

=== Component Descriptions

[cols="2,5", options="header"]
|===
| Component | Description

| *investigator.sh*
| Bash orchestrator that coordinates all analysis components, handles CLI arguments, manages system checks

| *sec_fundamental.py*
| Entry point for SEC analysis, uses pattern-based architecture via FundamentalAnalysisFacadeV2

| *yahoo_technical.py*
| Fetches market data from Yahoo Finance, calculates 30+ technical indicators, generates AI insights

| *synthesizer.py*
| Combines analyses with weighted scoring (60/40 split), generates investment recommendations and PDF reports

| *Pattern Architecture*
| Implements facades, strategies, and observer patterns for extensibility and separation of concerns

| *Cache Manager*
| Multi-backend cache system with priority-based retrieval: Memory(30) → Disk(10) → Database(5)

| *Ollama Integration*
| Local LLM server running llama3.1:8b-instruct-q8_0 for all analysis types

| *PostgreSQL*
| Stores analysis results, LLM responses, and cached data with JSONB compression
|===

[[data-flow]]
== 📊 Data Flow & Processing

=== Analysis Pipeline Overview

The InvestiGator analysis pipeline consists of three sequential stages:

1. **SEC Fundamental Analysis** (~2-5 minutes per stock)
   - Maps ticker symbol to CIK using SEC ticker.txt
   - Fetches company submissions and facts from SEC EDGAR
   - Extracts quarterly metrics via XBRL Frame API
   - Generates AI analysis for each filing period
   - Stores results in cache and database

2. **Technical Analysis** (~1-2 minutes per stock)
   - Fetches 365 days of price/volume data from Yahoo Finance
   - Calculates 30+ technical indicators
   - Identifies support/resistance levels and patterns
   - Generates AI-powered technical insights
   - Saves data as compressed Parquet files

3. **Investment Synthesis** (~30 seconds per stock)
   - Retrieves cached fundamental and technical analyses
   - Calculates weighted investment score (60% fundamental, 40% technical)
   - Generates comprehensive investment recommendation
   - Creates PDF report with charts and insights
   - Optionally sends email notification

=== Analysis Pipeline Sequence

[mermaid]
....
sequenceDiagram
    participant User
    participant Orchestrator
    participant TickerMapper
    participant SEC_API
    participant Fundamental
    participant Technical
    participant LLM
    participant Database
    participant Synthesizer
    participant Report

    User->>Orchestrator: ./investigator.sh --symbol AAPL
    Orchestrator->>TickerMapper: Get CIK for AAPL
    TickerMapper->>SEC_API: Download ticker.txt (if needed)
    TickerMapper-->>Orchestrator: CIK: 0000320193
    
    par Fundamental Analysis
        Orchestrator->>Fundamental: Analyze AAPL
        Fundamental->>Database: Check cache
        alt Cache miss
            Fundamental->>SEC_API: Get submissions
            Fundamental->>SEC_API: Get company facts
            Fundamental->>SEC_API: Get frame data
            Fundamental->>Database: Store raw data
        end
        Fundamental->>LLM: Analyze financials (per quarter)
        LLM->>Database: Store prompt/response (llm_type='sec')
        LLM-->>Fundamental: AI insights
    and Technical Analysis
        Orchestrator->>Technical: Analyze AAPL
        Technical->>LLM: Analyze indicators
        LLM->>Database: Store prompt/response (llm_type='ta')
        LLM-->>Technical: AI insights
    end
    
    Orchestrator->>Synthesizer: Combine analyses
    Synthesizer->>Database: Fetch LLM responses (sec + ta)
    Synthesizer->>LLM: Generate synthesis
    LLM->>Database: Store synthesis (llm_type='full')
    Synthesizer->>Report: Generate PDF
    Report-->>User: Investment Report
....

=== Data Processing Flow

[mermaid]
....
flowchart LR
    subgraph Input
        SYMBOL[Stock Symbol]
    end
    
    subgraph "Data Collection"
        CIK[CIK Lookup]
        SUB[SEC Submissions]
        FACTS[Company Facts]
        FRAME[Frame API Data]
        PRICE[Price Data]
    end
    
    subgraph "Processing"
        QPARSE[Quarterly Parsing]
        FPARSE[Filing Parser]
        TIND[Technical Indicators]
        SCORES[Score Calculation]
    end
    
    subgraph "AI Analysis"
        FLLM[Fundamental LLM]
        TLLM[Technical LLM]
        SLLM[Synthesis LLM]
    end
    
    subgraph "Storage"
        CACHE[(Cache)]
        DB[(Database)]
        FILES[File System]
    end
    
    subgraph Output
        REPORT[PDF Report]
        EMAIL[Email Alert]
    end
    
    SYMBOL --> CIK
    CIK --> SUB
    CIK --> FACTS
    CIK --> FRAME
    SYMBOL --> PRICE
    
    SUB --> QPARSE
    FACTS --> QPARSE
    FRAME --> QPARSE
    SUB --> FPARSE
    
    QPARSE --> FLLM
    FPARSE --> FLLM
    PRICE --> TIND
    TIND --> TLLM
    
    FLLM --> SLLM
    TLLM --> SLLM
    
    QPARSE --> CACHE
    FLLM --> DB
    TLLM --> DB
    SLLM --> DB
    
    FLLM --> FILES
    TLLM --> FILES
    SLLM --> FILES
    
    SLLM --> REPORT
    REPORT --> EMAIL
....

[[cache-management]]
== 💾 Cache Management System

=== Overview

InvestiGator features a sophisticated multi-level caching system designed for optimal performance and flexibility. The cache management system supports multiple storage backends with uniform compression and intelligent prioritization.

=== Cache Architecture

[mermaid]
....
graph TB
    subgraph "Cache Configuration"
        CONFIG["⚙️ config.json<br/>Cache Control"]
        STORAGE_LIST["📋 Storage List<br/>['disk', 'rdbms']"]
        CACHE_TYPES["🏷️ Cache Types<br/>Optional Filter"]
    end
    
    subgraph "Cache Manager"
        CM["🎯 CacheManager<br/>Central Controller"]
        HANDLER_REG["📝 Handler Registry<br/>Per Cache Type"]
        PRIORITY["🔢 Priority System<br/>Disk(10) > RDBMS(5)"]
    end
    
    subgraph "Storage Handlers"
        FILE_HANDLER["📁 FileCacheHandler<br/>JSON + gzip"]
        PARQUET_HANDLER["📊 ParquetHandler<br/>Compressed DataFrames"]
        RDBMS_HANDLER["🗄️ RDBMSHandler<br/>PostgreSQL JSONB"]
    end
    
    subgraph "Storage Backends"
        DISK["💾 Disk Storage<br/>data/cache_dirs/"]
        DATABASE["🗄️ PostgreSQL<br/>Hash Partitioned"]
    end
    
    subgraph "Cache Operations"
        READ["📖 Read Operation<br/>Priority Order"]
        WRITE["✍️ Write Operation<br/>All Backends"]
        CLEANUP["🧹 Cleanup<br/>Selective/Global"]
        INSPECT["🔍 Inspection<br/>Statistics & Health"]
    end
    
    CONFIG --> CM
    STORAGE_LIST --> CM
    CACHE_TYPES --> CM
    
    CM --> HANDLER_REG
    CM --> PRIORITY
    
    HANDLER_REG --> FILE_HANDLER
    HANDLER_REG --> PARQUET_HANDLER
    HANDLER_REG --> RDBMS_HANDLER
    
    FILE_HANDLER --> DISK
    PARQUET_HANDLER --> DISK
    RDBMS_HANDLER --> DATABASE
    
    CM --> READ
    CM --> WRITE
    CM --> CLEANUP
    CM --> INSPECT
    
    classDef config fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef manager fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef handler fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef storage fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef operation fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class CONFIG,STORAGE_LIST,CACHE_TYPES config
    class CM,HANDLER_REG,PRIORITY manager
    class FILE_HANDLER,PARQUET_HANDLER,RDBMS_HANDLER handler
    class DISK,DATABASE storage
    class READ,WRITE,CLEANUP,INSPECT operation
....

=== Cache Configuration

The cache system uses a list-based configuration approach for maximum flexibility:

[source,json]
----
{
  "cache_control": {
    "storage": ["disk", "rdbms"],     // Storage backends to use
    "types": null,                   // null = all types, or specific list
    "read_from_cache": true,         // Enable cache reads
    "write_to_cache": true,          // Enable cache writes
    "force_refresh": false,          // Global force refresh
    "force_refresh_symbols": null,   // Symbol-specific refresh
    "cache_ttl_override": null       // TTL override in hours
  },
  "parquet": {
    "compression": "gzip",           // Uniform compression
    "compression_level": 9          // Maximum compression
  }
}
----

=== Configuration Examples

[cols="3,4,3", options="header"]
|===
| Use Case | Configuration | Description

| *Production*
| `"storage": ["disk", "rdbms"]`
| Full redundancy with both storage backends

| *Development*
| `"storage": ["disk"]`
| Fast disk-only caching, no database dependency

| *No Cache*
| `"storage": []`
| Disable caching entirely, always fetch fresh data

| *SEC Only*
| `"storage": ["disk"], "types": ["sec"]`
| Cache only SEC data, skip LLM responses

| *Force Refresh*
| `"force_refresh_symbols": ["AAPL"]`
| Force refresh specific symbols only
|===

=== Cache Types & Handlers

[plantuml]
....
@startuml
title Cache Type to Handler Mapping

package "Cache Types" {
  enum CacheType {
    SEC_RESPONSE
    LLM_RESPONSE
    TECHNICAL_DATA
    SUBMISSION_DATA
    COMPANY_FACTS
    QUARTERLY_METRICS
  }
}

package "Storage Handlers" {
  class FileCacheHandler {
    +compression: gzip(9)
    +format: JSON
    +extension: .json.gz
    +priority: 10
  }
  
  class ParquetCacheHandler {
    +compression: gzip(9)
    +format: Parquet
    +extension: .parquet.gz
    +priority: 10
  }
  
  class RDBMSCacheHandler {
    +compression: JSONB/TOAST
    +storage: PostgreSQL
    +partitioning: Hash on CIK
    +priority: 5
  }
}

package "Storage Backends" {
  class DiskStorage {
    +path: data/cache_dirs/
    +compression_ratio: 70-80%
    +access_time: 10-50ms
  }
  
  class PostgreSQLStorage {
    +partitions: 10 (hash on CIK)
    +compression: Native TOAST
    +access_time: 50-200ms
  }
}

CacheType::SEC_RESPONSE --> FileCacheHandler
CacheType::LLM_RESPONSE --> FileCacheHandler
CacheType::COMPANY_FACTS --> FileCacheHandler
CacheType::QUARTERLY_METRICS --> FileCacheHandler

CacheType::TECHNICAL_DATA --> ParquetCacheHandler
CacheType::SUBMISSION_DATA --> ParquetCacheHandler

FileCacheHandler --> DiskStorage
ParquetCacheHandler --> DiskStorage
RDBMSCacheHandler --> PostgreSQLStorage

note right of FileCacheHandler : Used for JSON data\nwith gzip compression

note right of ParquetCacheHandler : Used for tabular data\nwith Parquet + gzip

note right of RDBMSCacheHandler : Used for all types\nwhen RDBMS enabled
@enduml
....

[[cache-structure]]

=== Cache Directory Structure

[source,text]
----
InvestiGator/
├── data/                           # Main data directory
│   ├── sec_cache/                  # SEC EDGAR cache
│   │   ├── ticker_cik_map.txt      # Symbol to CIK mappings
│   │   ├── {SYMBOL}/               # Per-symbol cache
│   │   │   ├── submissions.json    # Company filings list
│   │   │   ├── companyfacts.json   # XBRL company facts
│   │   │   ├── income_statement_*.json
│   │   │   ├── balance_sheet_*.json
│   │   │   ├── cash_flow_*.json
│   │   │   └── other_*.json
│   │   └── consolidated/           # Consolidated financial data
│   │       └── {SYMBOL}/
│   │           └── summary_*.json
│   │
│   └── llm_cache/                  # LLM prompts & responses
│       └── {SYMBOL}/
│           ├── prompt_10-K_FY.txt  # SEC fundamental prompts
│           ├── prompt_10-Q_Q*.txt  # Quarterly prompts
│           ├── response_10-K_FY.json # SEC LLM responses
│           ├── response_10-Q_Q*.json
│           ├── prompt_technical_indicators.txt # TA prompts
│           ├── response_technical_indicators.txt # TA responses
│           ├── prompt_synthesis.txt # Synthesis prompts
│           └── response_synthesis.txt # Final synthesis
│
├── logs/                           # Application logs
│   ├── investigator.log           # Main process log
│   ├── sec_fundamental.log        # SEC analysis log
│   ├── yahoo_technical.log        # Technical analysis log
│   └── synthesizer.log            # Report generation log
│
├── reports/                        # Generated reports
│   ├── synthesis/                  # Combined analysis reports
│   │   └── {SYMBOL}_investment_report_*.pdf
│   └── weekly/                     # Weekly batch reports
│       └── InvestiGator_Report_*.pdf
│
└── test_cache/                     # Test data cache
    └── {SYMBOL}_*.json
----

=== Database Schema

[mermaid]
....
erDiagram
    TICKER_CIK_MAPPING {
        varchar ticker PK
        varchar cik
        varchar company_name
        varchar exchange
        timestamp created_at
        timestamp updated_at
    }
    
    SEC_SUBMISSIONS {
        varchar cik PK
        varchar ticker
        varchar company_name
        jsonb filings
        jsonb recent_filings
        timestamp created_at
        timestamp updated_at
    }
    
    XBRL_COMPANY_FACTS {
        varchar cik PK
        varchar ticker
        jsonb facts "Complete XBRL data"
        timestamp created_at
        timestamp updated_at
    }
    
    QUARTERLY_METRICS {
        serial id PK
        varchar cik
        varchar ticker
        integer fiscal_year
        varchar fiscal_period
        varchar form_type
        varchar category
        jsonb concept_data
        jsonb common_metadata
        timestamp created_at
    }
    
    QUARTERLY_AI_SUMMARIES {
        serial id PK
        varchar cik
        varchar ticker
        integer fiscal_year
        varchar fiscal_period
        varchar form_type
        text financial_summary
        jsonb ai_analysis
        jsonb scores
        timestamp created_at
    }
    
    LLM_RESPONSE_STORE {
        uuid id PK
        varchar symbol
        varchar form_type "10-K/10-Q or N/A"
        varchar period "Q1/Q2/Q3/FY or N/A"
        jsonb prompt_context
        jsonb model_info
        jsonb response
        jsonb metadata
        varchar llm_type "sec/ta/full"
        timestamp ts
    }
    
    TICKER_CIK_MAPPING ||--o{ SEC_SUBMISSIONS : "maps to"
    SEC_SUBMISSIONS ||--o{ XBRL_COMPANY_FACTS : "enriches"
    XBRL_COMPANY_FACTS ||--o{ QUARTERLY_METRICS : "extracts"
    QUARTERLY_METRICS ||--o{ QUARTERLY_AI_SUMMARIES : "analyzed by"
    QUARTERLY_AI_SUMMARIES ||--o{ LLM_RESPONSE_STORE : "stored as"
    LLM_RESPONSE_STORE ||--o{ LLM_RESPONSE_STORE : "synthesizes"
....

=== Cache Strategy

[cols="3,2,3", options="header"]
|===
| Cache Type | TTL | Description

| *Ticker Mappings*
| 30 days
| SEC ticker.txt file cached locally

| *SEC Submissions*
| 7 days
| Company filing lists from EDGAR

| *Company Facts*
| 7 days
| Complete XBRL data for all periods

| *Frame Data*
| 24 hours
| Specific financial metrics by quarter

| *LLM Responses*
| Permanent
| All AI prompts and responses for debugging

| *Technical Data*
| Real-time
| No caching - always fetch fresh market data
|===

=== Cache Management Tools

InvestiGator provides comprehensive cache management utilities:

==== Command Line Interface

[source,bash]
----
# Cache inspection
./cache_manager.sh inspect           # Show all cache contents
./cache_manager.sh inspect AAPL      # Show cache for specific symbol
./cache_manager.sh size               # Show cache sizes

# Cache cleanup
./cache_manager.sh clean              # Clean all caches
./cache_manager.sh clean AAPL         # Clean cache for specific symbol
./cache_manager.sh clean-disk AAPL    # Clean disk cache only
./cache_manager.sh clean-db AAPL      # Clean database cache only

# Force refresh
./cache_manager.sh refresh AAPL       # Force refresh specific symbol

# Testing
./cache_manager.sh test               # Run cache tests
----

==== Python API

[source,python]
----
from utils.cache_cleanup import CacheCleanup
from utils.cache_inspector import CacheInspector
from config import get_config

# Initialize utilities
config = get_config()
cleanup = CacheCleanup(config)
inspector = CacheInspector(config)

# Cache inspection
inspector.print_cache_report()                    # Comprehensive report
summary_df = inspector.get_cache_summary('AAPL')  # DataFrame summary
disk_info = inspector.inspect_disk_cache('AAPL')  # Disk cache details
db_info = inspector.inspect_database_cache('AAPL') # Database cache details

# Cache cleanup
cleanup.clean_all_caches()                        # Clean everything
cleanup.clean_all_caches(symbol='AAPL')           # Clean specific symbol
cleanup.clean_disk_cache(symbol='AAPL')           # Disk only
cleanup.truncate_cache_tables(symbol='AAPL')      # Database only
----

=== Cache Performance Metrics

[cols="3,2,3,2", options="header"]
|===
| Operation | Disk Cache | Database Cache | Compression Ratio

| *Read (Small)*
| 10-30ms
| 50-100ms
| N/A

| *Read (Large)*
| 30-80ms
| 100-300ms
| N/A

| *Write (JSON)*
| 20-50ms
| 80-200ms
| 70-80%

| *Write (Parquet)*
| 50-150ms
| N/A
| 60-75%

| *Storage Efficiency*
| gzip level 9
| JSONB TOAST
| 65-80%
|===

=== Cache Flow Diagram

[mermaid]
....
flowchart TD
    START(["Cache Operation Request"]) --> CONFIG_CHECK{"Check Cache Config"}
    
    CONFIG_CHECK -->|"storage: []"| NO_CACHE["❌ Skip Cache<br/>Return None"]
    CONFIG_CHECK -->|"storage: ['disk']"| DISK_ONLY["💾 Disk Only"]
    CONFIG_CHECK -->|"storage: ['rdbms']"| DB_ONLY["🗄️ Database Only"]
    CONFIG_CHECK -->|"storage: ['disk','rdbms']"| BOTH_STORAGE["🔄 Both Storage"]
    
    subgraph DISK_FLOW ["Disk Cache Flow"]
        DISK_READ["📖 Read from Disk"]
        DISK_HIT{"Cache Hit?"}
        DISK_WRITE["💾 Write to Disk"]
        DISK_COMPRESS["🗜️ gzip Compression"]
    end
    
    subgraph DB_FLOW ["Database Cache Flow"]
        DB_READ["📊 Read from Database"]
        DB_HIT{"Cache Hit?"}
        DB_WRITE["🗄️ Write to Database"]
        DB_COMPRESS["📦 JSONB Compression"]
    end
    
    subgraph PRIORITY_FLOW ["Priority Resolution"]
        CHECK_FORCE{"Force Refresh?"}
        CHECK_SYMBOL{"Symbol-Specific<br/>Force Refresh?"}
        PRIORITY_READ["🏆 Priority Read<br/>Disk → Database"]
        PROMOTE["⬆️ Promote to Disk"]
    end
    
    DISK_ONLY --> DISK_FLOW
    DB_ONLY --> DB_FLOW
    BOTH_STORAGE --> PRIORITY_FLOW
    
    DISK_READ --> DISK_HIT
    DISK_HIT -->|"Hit"| RETURN_DISK["✅ Return Data"]
    DISK_HIT -->|"Miss"| FETCH_FRESH["🔄 Fetch Fresh Data"]
    
    DB_READ --> DB_HIT
    DB_HIT -->|"Hit"| RETURN_DB["✅ Return Data"]
    DB_HIT -->|"Miss"| FETCH_FRESH
    
    CHECK_FORCE -->|"Yes"| FETCH_FRESH
    CHECK_FORCE -->|"No"| CHECK_SYMBOL
    CHECK_SYMBOL -->|"Yes"| FETCH_FRESH
    CHECK_SYMBOL -->|"No"| PRIORITY_READ
    
    PRIORITY_READ --> DISK_READ
    RETURN_DB --> PROMOTE
    PROMOTE --> DISK_WRITE
    
    FETCH_FRESH --> DISK_WRITE
    FETCH_FRESH --> DB_WRITE
    DISK_WRITE --> DISK_COMPRESS
    DB_WRITE --> DB_COMPRESS
    
    DISK_COMPRESS --> SUCCESS["✅ Cache Updated"]
    DB_COMPRESS --> SUCCESS
    RETURN_DISK --> SUCCESS
    NO_CACHE --> SUCCESS
    
    classDef decision fill:#fff2cc,stroke:#d6b656,stroke-width:2px
    classDef process fill:#d5e8d4,stroke:#82b366,stroke-width:2px
    classDef storage fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px
    classDef success fill:#e1d5e7,stroke:#9673a6,stroke-width:2px
    
    class CONFIG_CHECK,DISK_HIT,DB_HIT,CHECK_FORCE,CHECK_SYMBOL decision
    class DISK_READ,DB_READ,PRIORITY_READ,FETCH_FRESH,PROMOTE process
    class DISK_WRITE,DB_WRITE,DISK_COMPRESS,DB_COMPRESS storage
    class SUCCESS,RETURN_DISK,RETURN_DB,NO_CACHE success
....

=== LLM Observability

The system maintains comprehensive logs of all LLM interactions:

[source,json]
----
// Example: data/llm_cache/AAPL/response_10-Q_Q1.json
{
  "type": "text",
  "content": "Based on Apple's Q1 2024 financial data...",
  "metadata": {
    "processing_time_ms": 15234,
    "response_length": 3425,
    "timestamp": "2025-01-28T10:15:32",
    "model": "phi4-reasoning",
    "scores": {
      "financial_health": 8.5,
      "growth_prospects": 7.8,
      "business_quality": 9.0
    }
  }
}
----

[[prerequisites]]
== 📋 Prerequisites

=== Hardware Requirements
* *macOS 12.0+* with Apple Silicon (M1/M2/M3)
* *RAM Requirements*:
  - Minimum: 32GB (runs lighter models)
  - Recommended: 64GB+ (runs full-size models)
* *Storage*: 200GB+ free space
  - AI Models: ~60GB
  - Database: ~1GB (grows over time)
  - Reports & Cache: ~10GB

=== Software Requirements
* *Python 3.9+*
* *PostgreSQL 14+*
* *Homebrew* (for package management)
* *Active internet connection* (for data fetching)

[[quick-start]]
== 🚀 Quick Start

Get InvestiGator running in minutes:

[source,bash]
----
# Clone the repository
git clone https://github.com/vjsingh1984/InvestiGator.git
cd InvestiGator

# Make the main script executable
chmod +x investigator.sh

# Install system dependencies (macOS with Homebrew)
brew install postgresql@14 python@3.9

# Create Python virtual environment
python3 -m venv venv
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt

# Install Ollama (for local LLM)
curl -fsSL https://ollama.com/install.sh | sh

# Pull the default LLM model
ollama pull llama3.1:8b-instruct-q8_0

# Configure the system (edit config.json)
# Set SEC user agent: "user_agent": "YourName/1.0 (your-email@example.com)"

# Set up PostgreSQL database
createdb investment_ai
psql -d investment_ai -f schema/consolidated_schema.sql

# Test the system
./investigator.sh --test-system

# Analyze your first stock
./investigator.sh --symbol AAPL
----

That's it! InvestiGator is now analyzing Apple Inc. and will generate a comprehensive investment report in the `reports/synthesis/` directory.

[[detailed-setup]]
== 📖 Detailed Setup Guide

=== Step 1: System Dependencies

The setup script automatically installs all required dependencies:

[source,bash]
----
# Clone and setup
git clone https://github.com/vjsingh1984/InvestiGator.git
cd InvestiGator

# Make setup script executable
chmod +x scripts/setup.sh

# Run setup (includes all dependencies)
./scripts/setup.sh
----

What gets installed:

[source,bash]
----
# Homebrew packages
brew install postgresql@14 python@3.9 git curl wget

# Python packages (via pip)
sqlalchemy==2.0.23        # Database ORM
psycopg2-binary==2.9.9    # PostgreSQL adapter
pandas==2.1.4             # Data manipulation
requests==2.31.0          # HTTP requests
yfinance                  # Yahoo Finance data
reportlab==4.0.7          # PDF generation
schedule==1.2.1           # Task scheduling
lxml==4.9.3              # XML parsing
beautifulsoup4==4.12.2   # HTML parsing
numpy                     # Numerical computing
python-dateutil          # Date utilities

# Ollama for AI models
curl -fsSL https://ollama.com/install.sh | sh
----

=== Step 2: Database Setup

PostgreSQL database is automatically configured:

[source,bash]
----
# Database created automatically by setup script
Database: investment_ai
User: investment_user
Password: investment_pass

# Apply complete database schema
psql -U investment_user -d investment_ai -f utils/database_schema.sql

# Manual verification (optional)
psql -U investment_user -d investment_ai -c "\dt"
----

Database features:
* *Hash partitioning* on CIK for performance
* *Optimized indexes* for common queries
* *LLM response tracking* with observability
* *Automatic maintenance* tasks

=== Step 3: AI Model Installation

InvestiGator automatically downloads optimized models based on your Mac's RAM:

*For 64GB+ MacBooks (Recommended):*
[source,bash]
----
# High-performance models
ollama pull phi4-reasoning                  # Fundamental analysis (16GB)
ollama pull qwen2.5:32b-instruct-q4_K_M    # Technical analysis (20GB)
ollama pull llama-3.3-70b-instruct-q4_k_m  # Report synthesis (40GB)
----

*For 32GB MacBooks:*
[source,bash]
----
# Optimized models
ollama pull phi3:14b-medium-4k-instruct-q4_1  # Fundamental (8GB)
ollama pull mistral:v0.3                       # Technical (4GB)
ollama pull llama3.1:8b                        # Synthesis (5GB)
----

=== Step 4: Configuration

Edit `config.json` to configure InvestiGator:

[source,json]
----
{
  "sec": {
    "user_agent": "YourName/1.0 (your-email@example.com)"  // REQUIRED
  },
  "email": {
    "enabled": true,
    "username": "your-email@gmail.com",
    "password": "your-app-password",     // Gmail App Password
    "recipients": ["your-email@gmail.com"]
  },
  "stocks_to_track": [
    "AAPL", "GOOGL", "MSFT", "AMZN"    // Your portfolio
  ]
}
----

[[usage]]
== 📚 Usage Guide

=== Basic Commands

[source,bash]
----
# Analyze a single stock
./investigator.sh --symbol AAPL

# Analyze multiple stocks (batch mode)
./investigator.sh --symbols AAPL GOOGL MSFT NVDA

# Generate weekly portfolio report
./investigator.sh --weekly-report

# Weekly report with email delivery
./investigator.sh --weekly-report --send-email

# Test system components
./investigator.sh --test-system

# Display system statistics
./investigator.sh --system-stats

# Start automated scheduler (weekly reports)
./investigator.sh --start-scheduler

# View comprehensive help
./investigator.sh --help
----

=== Advanced Usage

[source,bash]
----
# Cache management
./investigator.sh --clean-cache --symbol AAPL      # Clean specific symbol
./investigator.sh --clean-cache-all                # Clean all caches
./investigator.sh --inspect-cache                   # View cache contents
./investigator.sh --cache-sizes                     # Show cache statistics
./investigator.sh --force-refresh --symbol AAPL    # Force data refresh

# Direct module execution
python sec_fundamental.py --symbol AAPL            # SEC analysis only
python yahoo_technical.py --symbol AAPL            # Technical analysis only
python synthesizer.py --symbol AAPL --report       # Generate report only

# Testing and debugging
./investigator.sh --test-cache                     # Test cache system
./investigator.sh --run-tests unit                 # Run unit tests
./investigator.sh --run-tests coverage             # Generate coverage report
./investigator.sh --debug                          # Enable debug logging
----

=== Monitoring & Logs

[source,bash]
----
# Real-time log monitoring
tail -f logs/investigator.log

# Check for errors
grep ERROR logs/*.log

# Monitor LLM performance
grep "processing_time_ms" data/llm_cache/*/response_*.json

# Database queries
psql -U investment_user -d investment_ai -c "
  SELECT symbol, llm_type, form_type, period, ts 
  FROM llm_response_store 
  ORDER BY ts DESC 
  LIMIT 10;
"
----

[[configuration]]
== ⚙️ Configuration

=== Cache Configuration

Configure the cache management system in `config.json`:

[source,json]
----
{
  "cache_control": {
    "storage": ["disk", "rdbms"],     // Storage backends
    "types": null,                   // Cache types filter
    "read_from_cache": true,         // Enable reads
    "write_to_cache": true,          // Enable writes
    "force_refresh": false,          // Global force refresh
    "force_refresh_symbols": null,   // Symbol-specific refresh
    "cache_ttl_override": null       // TTL override (hours)
  },
  "parquet": {
    "compression": "gzip",           // Compression algorithm
    "compression_level": 9          // Compression level (1-9)
  }
}
----

=== Core Configuration (config.json)

[source,json]
----
{
  "database": {
    "host": "localhost",
    "port": 5432,
    "database": "investment_ai",
    "username": "investment_user",
    "password": "investment_pass"
  },
  
  "ollama": {
    "base_url": "http://localhost:11434",
    "timeout": 300,
    "models": {
      "fundamental_analysis": "phi4-reasoning",
      "technical_analysis": "qwen2.5:32b-instruct-q4_K_M",
      "report_generation": "llama-3.3-70b-instruct-q4_k_m"
    }
  },
  
  "sec": {
    "user_agent": "YourName/1.0 (email@example.com)",
    "rate_limit": 10,
    "cache_dir": "./data/sec_cache"
  },
  
  "analysis": {
    "fundamental_weight": 0.6,
    "technical_weight": 0.4,
    "min_score_for_buy": 7.0,
    "max_score_for_sell": 4.0
  }
}
----

=== Environment Variables

[source,bash]
----
# Optional overrides
export DB_HOST=localhost
export DB_PORT=5432
export DB_USER=investment_user
export DB_PASSWORD=secure_password
export OLLAMA_URL=http://localhost:11434
export EMAIL_PASSWORD=gmail_app_password
----

[[testing]]
== 🧪 Testing

InvestiGator includes comprehensive test coverage:

[source,bash]
----
# Run all tests
./run_tests.sh

# Run specific test suites
./run_tests.sh unit          # Unit tests
./run_tests.sh integration   # Integration tests
./run_tests.sh sec           # SEC EDGAR tests
./run_tests.sh coverage      # Generate coverage report

# Test individual components
python -m pytest tests/test_sec_fundamental.py -v
python -m pytest tests/test_yahoo_technical.py -v
python -m pytest tests/test_synthesizer.py -v

# Test cache management system
python -m pytest tests/test_cache_management.py -v

# Test cache configurations
./cache_manager.sh test

# Performance benchmarking
python benchmark_cache.py
----

[[troubleshooting]]
== 🔧 Troubleshooting

=== Common Issues

*Ticker Not Found*
[source,bash]
----
# Refresh ticker mappings
python -c "from utils.ticker_cik_mapper import TickerMapper; TickerMapper().refresh_mapping()"
----

*Model Loading Errors*
[source,bash]
----
# Check Ollama status
ollama list
systemctl status ollama  # Linux
brew services list       # macOS

# Restart Ollama
brew services restart ollama
----

*Database Connection Issues*
[source,bash]
----
# Check PostgreSQL status
pg_ctl -D /opt/homebrew/var/postgresql@14 status

# Restart PostgreSQL
brew services restart postgresql@14
----

*Cache Issues*
[source,bash]
----
# Use cache management tools
./cache_manager.sh clean            # Clean all caches safely
./cache_manager.sh clean AAPL       # Clean specific symbol
./cache_manager.sh inspect          # Check cache status

# Manual cleanup (not recommended)
rm -rf data/sec_cache/*
rm -rf data/llm_cache/*

# Database cache cleanup
psql -U investment_user -d investment_ai -c "TRUNCATE TABLE sec_response_store;"
----

=== Debug Mode

[source,bash]
----
# Enable verbose logging
export LOG_LEVEL=DEBUG
./investigator.sh --symbol AAPL --verbose

# Check system resources
./investigator.sh --test-system --verbose
----

[[optimization]]
== ⚡ Performance Optimization Opportunities

=== Current Bottlenecks
1. *Sequential LLM Processing*: Single-threaded execution limits throughput
2. *Large Context Windows*: 32K context for synthesis is resource-intensive
3. *Memory Usage*: Full DataFrames loaded for technical analysis
4. *Pattern Complexity*: Deep nesting of facades and strategies adds overhead

=== Recommended Optimizations

==== Short-term (Quick Wins)
* *Batch Technical Indicators*: Calculate multiple indicators in parallel using vectorized operations
* *Cache Preloading*: Warm cache for frequently accessed symbols during startup
* *Reduce Context Size*: Optimize prompts to use smaller context windows (8-16K)
* *Add Progress Indicators*: Show analysis progress to improve user experience

==== Medium-term (Architecture)
* *Async I/O*: Use asyncio for API calls and database operations
* *Connection Pooling*: Implement pooling for SEC API and Yahoo Finance requests
* *Streaming Processing*: Process large datasets in chunks rather than loading entirely
* *LLM Model Switching*: Use smaller models for simple tasks, larger for complex analysis

==== Long-term (Scalability)
* *Parallel Symbol Processing*: Analyze multiple stocks concurrently with resource limits
* *Distributed Cache*: Consider Redis for shared caching across processes
* *Queue-based Architecture*: Implement job queues for better resource management
* *Monitoring & Metrics*: Add performance tracking and alerting

=== Configuration Tuning

[source,json]
----
{
  "ollama": {
    "num_predict": {
      "fundamental_analysis": 1024,  // Reduce from 2048
      "technical_analysis": 512,     // Reduce from 1024
      "synthesis": 1536              // Reduce from 2048
    }
  },
  "cache_control": {
    "storage": ["disk"],  // Use disk-only for development
    "types": ["sec", "llm", "technical"]  // Enable selective caching
  }
}
----

[[security]]
== 🔒 Security & Privacy

=== Data Protection
* All processing happens locally on your machine
* No data is sent to external services (except SEC EDGAR and Yahoo Finance)
* Database credentials are stored locally in config.json
* Email passwords use app-specific tokens
* Cache data is compressed but not encrypted

=== Best Practices
1. Use strong database passwords
2. Enable FileVault on macOS for disk encryption
3. Use Gmail App Passwords (not regular passwords)
4. Regularly update dependencies with `pip install -U`
5. Monitor log files for anomalies
6. Restrict config.json permissions: `chmod 600 config.json`

=== Cache Management Best Practices
1. *Development*: Use disk-only caching (`"storage": ["disk"]`)
2. *Production*: Use both storage backends for redundancy
3. *Testing*: Disable caching or use short TTL overrides
4. *Debugging*: Use force refresh for specific symbols
5. *Maintenance*: Schedule regular cache cleanup
6. *Performance*: Monitor cache hit rates and compression ratios
7. *Disk Space*: Regularly inspect cache sizes with `./cache_manager.sh size`

[[project-structure]]
== 📁 Project Structure

[source,text]
----
InvestiGator/
├── investigator.sh          # Main entry point (Bash orchestrator)
├── sec_fundamental.py       # SEC filing analysis module
├── yahoo_technical.py       # Technical analysis module
├── synthesizer.py          # Report synthesis module
├── config.py               # Configuration management
├── config.json             # User configuration file
├── requirements.txt        # Python dependencies
│
├── patterns/               # Pattern-based architecture
│   ├── core/              # Core interfaces and base classes
│   ├── llm/               # LLM facades and strategies
│   └── sec/               # SEC analysis patterns
│
├── utils/                  # Utility modules
│   ├── cache/             # Multi-backend cache system
│   ├── api_client.py      # HTTP client utilities
│   ├── db.py              # Database management
│   ├── financial_data_aggregator.py
│   ├── prompt_manager.py  # LLM prompt templates
│   ├── report_generator.py # PDF report generation
│   ├── sec_frame_api.py   # SEC XBRL Frame API client
│   └── ticker_cik_mapper.py # Ticker to CIK mapping
│
├── data/                   # Data storage
│   ├── sec_cache/         # SEC API responses
│   ├── llm_cache/         # LLM prompts and responses
│   ├── technical_cache/   # Technical analysis data
│   └── price_cache/       # Price history (Parquet)
│
├── reports/                # Generated reports
│   ├── synthesis/         # Investment reports (PDF)
│   └── weekly/            # Weekly portfolio reports
│
├── logs/                   # Application logs
├── prompts/                # Jinja2 prompt templates
└── schema/                 # Database schema files
----

[[contributing]]
== 🤝 Contributing

We welcome contributions! Please see our link:CONTRIBUTING.md[Contributing Guide] for details.

=== Development Setup

[source,bash]
----
# Fork and clone
git clone https://github.com/YOUR_USERNAME/InvestiGator.git
cd InvestiGator

# Create branch
git checkout -b feature/your-feature

# Set up development environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run tests
./investigator.sh --run-tests all

# Submit PR
git push origin feature/your-feature
----

=== Contribution Areas

* **Performance Optimization**: Help implement the optimizations listed in the Performance section
* **New Indicators**: Add more technical analysis indicators
* **UI/UX**: Create a web interface or improve CLI experience
* **Documentation**: Improve docs, add tutorials, create video guides
* **Testing**: Add unit tests, integration tests, performance benchmarks
* **Model Support**: Add support for more LLM models and providers

[[license]]
== 📄 License

InvestiGator is licensed under the Apache License, Version 2.0. See link:LICENSE[LICENSE] for details.

* Free for personal, educational, and commercial use
* No restrictions on commercial deployment
* No time limitations or usage fees

---

*Built with ❤️ by investors, for investors*

_Star this repo if you find it useful!_